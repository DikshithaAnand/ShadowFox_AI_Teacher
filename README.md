# ğŸ¤– AI Teacher â€“ Local AI Topic Explainer

AI Teacher is a **local, offline AI-powered topic explainer** built using **Streamlit** and **Ollama**.  
It uses a lightweight **Phi language model** to generate fast and clear explanations without relying on cloud APIs or internet access after setup.

This project is suitable for **learning, internships, mini projects, and demonstrations**.

---

## ğŸš€ Features

- ğŸ§  Local AI model (no cloud or API keys)
- âš¡ Fast responses optimized for CPU
- ğŸ”’ Secure (no secrets or tokens required)
- ğŸŒ Works offline after setup
- ğŸ§‘â€ğŸ“ Ideal for explaining topics and concepts
- ğŸ’» Simple and clean Streamlit interface

---

## ğŸ› ï¸ Tech Stack

- **Frontend:** Streamlit  
- **Backend:** Python  
- **LLM:** Phi model  
- **Inference Engine:** Ollama (local HTTP Api)

---

## ğŸ“ Project Structure
```text
ai-assistant/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ llm_api.py
â”‚
â”œâ”€â”€ frontend/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Install Ollama
Download from:
https://ollama.com/download

Verify installation:
```bash
ollama --version
```

## 2ï¸âƒ£ Install Phi Model

Download the lightweight Phi language model using Ollama:

```bash
ollama pull phi
```

## 3ï¸âƒ£ Create Virtual Environment (Optional)

It is recommended to use a virtual environment for dependency management.

```bash
python -m venv venv
venv\Scripts\activate
```

## 4ï¸âƒ£ Install Dependencies

Install the required Python packages:

```bash
pip install -r requirements.txt
```

## 5ï¸âƒ£ Run the Application

Start the Streamlit application using:

```bash
streamlit run app.py
```

The application will open in your default web browser.

# ğŸ§ª Example Prompts

You can ask the AI Assistant the following types of questions:

- Explain the libraries in Python  
- What is machine learning?  
- Explain Indiaâ€™s economy in simple terms  
- Write a Python function to reverse a list  

These prompts demonstrate the systemâ€™s ability to explain concepts and generate simple code snippets.

# âš¡ Performance Notes

- Uses the **Phi language model (~1.6 GB)** for fast inference  
- Runs completely **offline** after initial setup  
- The model is kept **in memory** using Ollamaâ€™s local server  
- Optimized for **CPU-only systems**, suitable for laptops and low-resource machines  

This design ensures low latency and consistent performance.

# ğŸ“ Use Cases

- Topic explanation tool for students  
- Offline AI-based learning assistant  
- Internship or academic mini project  
- Demonstration of AI fundamentals and local LLM deployment  

The project is suitable for both educational and demonstrative purposes.

# âš ï¸ Limitations

- Knowledge cutoff depends on the model training data (approximately 2023)  
- Does not provide real-time or live internet-based information  
- Designed mainly for explanations, not large-scale or production applications  

These limitations are expected for local, offline language models.

# ğŸ“Œ Future Enhancements

- Add a model selector (Phi / Mistral)  
- Introduce an Explanation vs Code output toggle  
- Implement chat history support  
- Improve UI design and theming  
- Add options to export or save responses  

These enhancements can further improve usability and functionality.

## ğŸ‘¤ Author

**Dikshitha Anand**

- AI & Machine Learning Enthusiast  
- Interested in Python, Data Structures, and AI-based applications  
- Focused on building practical AI tools using open-source technologies

**GitHub:** https://github.com/DikshithaAnand

